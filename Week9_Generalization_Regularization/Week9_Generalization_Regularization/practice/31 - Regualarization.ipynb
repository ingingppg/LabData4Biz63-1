{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Regularization\n",
    "\n",
    "Term 1 2020 - Instructor: Teerapong Leelanupab\n",
    "\n",
    "Teaching Assistant: \n",
    "1. Tiwipab Meephruek (Mil)\n",
    "2. Jiratkul Wangsiripaisarn (Brooklyn)\n",
    "3. Hataichanok Sakkara (Pond)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.sort(np.random.rand(100))\n",
    "y = np.cos(1.2 * x * np.pi) + (0.1 * np.random.randn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The random numbers are added to y so as to generate noise, since the real world data wont be fitting a line exactly and will have some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total data is split into train and test data. Test data is 20% of entire data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.scatter(X_train, Y_train, color = 'k', label = 'Train data')\n",
    "plt.scatter(X_test, Y_test, color = 'r', label = 'Test data')\n",
    "plt.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True-fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The train data and test data is plotted.\n",
    "- The true fit line is the actual function from which we generated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an equation of degree 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will be using an hyothesis made from an equatio of degree 1 for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(-1,1)\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train, Y_train)\n",
    "train_accuracy = clf.score(x_train, Y_train)\n",
    "print('train accuracy', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X_test.reshape(-1,1)\n",
    "test_accuracy = clf.score(x_test, Y_test)\n",
    "print('test accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = clf.predict(x_train)\n",
    "train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "print('Training MSE:', train_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = clf.predict(x_test)\n",
    "test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "print('Test MSE:', test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax1.plot(X_test, test_predict, label = 'Model function' )\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "plt.legend()\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model is under-fit as it is not able to fit the training examples correctly. \n",
    "- The training and testing error are also quite high.\n",
    "- Increasing the degree of the equation may do the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an equation of degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(-1,1)\n",
    "transf = preprocessing.PolynomialFeatures()\n",
    "x_train = transf.fit_transform(x_train)\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train, Y_train)\n",
    "train_accuracy = clf.score(x_train, Y_train)\n",
    "print('Train accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X_test.reshape(-1,1)\n",
    "transf = preprocessing.PolynomialFeatures()\n",
    "x_test = transf.fit_transform(x_test)\n",
    "test_accuracy = clf.score(x_test, Y_test)\n",
    "print('test accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = clf.predict(x_train)\n",
    "train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "print('Training MSE:', train_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = clf.predict(x_test)\n",
    "test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "print('Test MSE:', test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = x.reshape(-1,1)\n",
    "x_model = transf.fit_transform(x_model)\n",
    "y_model = clf.predict(x_model)\n",
    "x_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "plt.legend()\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2nd degree equation fits the data better when compared to 1st degree equation.\n",
    "- The errors have been reduced significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an equation of degree 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(-1,1)\n",
    "transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "x_train = transf.fit_transform(x_train)\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train, Y_train)\n",
    "train_accuracy = clf.score(x_train, Y_train)\n",
    "print('Train accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X_test.reshape(-1,1)\n",
    "transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "x_test = transf.fit_transform(x_test)\n",
    "test_accuracy = clf.score(x_test, Y_test)\n",
    "print('test accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = clf.predict(x_train)\n",
    "train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "print('Training MSE:', train_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = clf.predict(x_test)\n",
    "test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "print('Test MSE:', test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = x.reshape(-1,1)\n",
    "x_model = transf.fit_transform(x_model)\n",
    "y_model = clf.predict(x_model)\n",
    "x_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "plt.legend()\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model function is very much distorted as its degree is high which makes it flexible enough to try to pass thorugh all the training examples.\n",
    "- As the training data has some amount of noise, it will end up capturing that noise and will be misled by that noise when it tries to make predictions on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The optimum model would be the one fitting the data without under-fitting or over-fitting it.\n",
    "- Such an optimal model can be decided from the scores of Cross-validation and also by checking the MSE of the models using equations of different degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking MSE for different degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(degree):\n",
    "    x_train = X_train.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = degree)\n",
    "    x_train = transf.fit_transform(x_train)\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(x_train, Y_train)\n",
    "    train_accuracy = clf.score(x_train, Y_train)\n",
    "    x_test = X_test.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = degree)\n",
    "    x_test = transf.fit_transform(x_test)\n",
    "    test_accuracy = clf.score(x_test, Y_test)\n",
    "    train_predict = clf.predict(x_train)\n",
    "    train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "    test_predict = clf.predict(x_test)\n",
    "    test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "    return train_accuracy, test_accuracy, train_MSE, test_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_acc = []\n",
    "Test_acc = []\n",
    "Train_MSE = []\n",
    "Test_MSE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    a, b, c, d = Evaluation(i+1)\n",
    "    Train_acc.append(a)\n",
    "    Test_acc.append(b)\n",
    "    Train_MSE.append(c)\n",
    "    Test_MSE.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.linspace(1, 40, 40)\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.plot(degrees, Train_acc, label = 'Training accuracy', linewidth = 3)\n",
    "ax1.plot(degrees, Test_acc, label = 'Testing accuracy', linewidth = 3)\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Degrees')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax2.plot(degrees, Train_MSE, label = 'Training MSE', linewidth = 3)\n",
    "ax2.plot(degrees, Test_MSE, label = 'Testing MSE', linewidth = 3)\n",
    "plt.ylim(0, 0.05)\n",
    "plt.legend()\n",
    "ax2.set_xlabel('Degrees')\n",
    "ax2.set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_min_degree = Test_MSE.index(min(Test_MSE)) + 1\n",
    "print('Minimum test error occurs at degree', Test_min_degree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_min_degree = Train_MSE.index(min(Train_MSE)) + 1\n",
    "print('Minimum training error occurs at degree', Train_min_degree )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have calculated the MSE and accuracy scores of the model while using equation upto degree 40. We could find that the model has least testing error at degree 9 and least training error at degree 27.\n",
    "- The testing error rises after degree 9 as the model begins to overfit and it cannot predict the test values correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PLotting the model using an equation of degree 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(-1,1)\n",
    "transf = preprocessing.PolynomialFeatures(degree = 9)\n",
    "x_train = transf.fit_transform(x_train)\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train, Y_train)\n",
    "train_accuracy = clf.score(x_train, Y_train)\n",
    "print('Train accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X_test.reshape(-1,1)\n",
    "transf = preprocessing.PolynomialFeatures(degree = 9)\n",
    "x_test = transf.fit_transform(x_test)\n",
    "test_accuracy = clf.score(x_test, Y_test)\n",
    "print('test accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = clf.predict(x_train)\n",
    "train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "print('Training MSE:', train_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = clf.predict(x_test)\n",
    "test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "print('Test MSE:', test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = x.reshape(-1,1)\n",
    "x_model = transf.fit_transform(x_model)\n",
    "y_model = clf.predict(x_model)\n",
    "x_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "plt.legend()\n",
    "ax2.set_xlabel('Degrees')\n",
    "ax2.set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization to prevent over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The two types of regularization are\n",
    "1. L1 regularization or LASSO regression\n",
    "2. L2 regularization or Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will fit a Ridge regression moel on the above data with various lambda values and see the effect of changing lambda on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_reg(lamda):\n",
    "    x_train = X_train.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_train = transf.fit_transform(x_train)\n",
    "    clf = Ridge(alpha = lamda)\n",
    "    clf.fit(x_train, Y_train)\n",
    "    train_accuracy = clf.score(x_train, Y_train)\n",
    "    intercept = clf.intercept_\n",
    "    coefficient = clf.coef_\n",
    "    parameters = coefficient + intercept\n",
    "    x_test = X_test.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_test = transf.fit_transform(x_test)\n",
    "    test_accuracy = clf.score(x_test, Y_test)\n",
    "    train_predict = clf.predict(x_train)\n",
    "    train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "    test_predict = clf.predict(x_test)\n",
    "    test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "    print('Train accuracy:', train_accuracy, '\\n')\n",
    "    print('Test accuracy:', test_accuracy, '\\n')\n",
    "    print('Train MSE', train_MSE, '\\n')\n",
    "    print('Test MSE', test_MSE, '\\n')\n",
    "    print('Parameters:', parameters)\n",
    "    x_model = x.reshape(-1,1)\n",
    "    x_model = transf.fit_transform(x_model)\n",
    "    y_model = clf.predict(x_model)\n",
    "    x_test = X_test\n",
    "    fig = plt.figure(figsize = (20,10))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "    ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "    ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "    plt.legend()\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(0.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that as the value of lambda increases the model becomes a straight line parallel to x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will fit a LASSO regression model on above data and see the effect of change in lambda on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_reg(lamda):\n",
    "    x_train = X_train.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_train = transf.fit_transform(x_train)\n",
    "    clf = Lasso(alpha = lamda)\n",
    "    clf.fit(x_train, Y_train)\n",
    "    intercept = clf.intercept_\n",
    "    coefficient = clf.coef_\n",
    "    parameters = coefficient + intercept\n",
    "    train_accuracy = clf.score(x_train, Y_train)\n",
    "    x_test = X_test.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_test = transf.fit_transform(x_test)\n",
    "    test_accuracy = clf.score(x_test, Y_test)\n",
    "    train_predict = clf.predict(x_train)\n",
    "    train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "    test_predict = clf.predict(x_test)\n",
    "    test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "    print('Train accuracy:', train_accuracy, '\\n')\n",
    "    print('Test accuracy:', test_accuracy, '\\n')\n",
    "    print('Train MSE', train_MSE, '\\n')\n",
    "    print('Test MSE', test_MSE, '\\n')\n",
    "    print('Parameters:', parameters)\n",
    "    x_model = x.reshape(-1,1)\n",
    "    x_model = transf.fit_transform(x_model)\n",
    "    y_model = clf.predict(x_model)\n",
    "    x_test = X_test\n",
    "    fig = plt.figure(figsize = (20,10))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "    ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "    ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "    plt.legend()\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here I am using the RidgeCV regressor from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_reg_cv(lamda):\n",
    "    x_train = X_train.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_train = transf.fit_transform(x_train)\n",
    "    clf = RidgeCV(alphas = lamda, cv = 5)\n",
    "    clf.fit(x_train, Y_train)\n",
    "    best_alpha = clf.alpha_\n",
    "    cv_score = clf.best_score_\n",
    "    train_accuracy = clf.score(x_train, Y_train)\n",
    "    intercept = clf.intercept_\n",
    "    coefficient = clf.coef_\n",
    "    parameters = coefficient + intercept\n",
    "    x_test = X_test.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_test = transf.fit_transform(x_test)\n",
    "    test_accuracy = clf.score(x_test, Y_test)\n",
    "    train_predict = clf.predict(x_train)\n",
    "    train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "    test_predict = clf.predict(x_test)\n",
    "    test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "    print('Best lambda:', best_alpha)\n",
    "    print('CV score:', cv_score)\n",
    "    print('Train accuracy:', train_accuracy, '\\n')\n",
    "    print('Test accuracy:', test_accuracy, '\\n')\n",
    "    print('Train MSE', train_MSE, '\\n')\n",
    "    print('Test MSE', test_MSE, '\\n')\n",
    "    print('Parameters:', parameters)\n",
    "    x_model = x.reshape(-1,1)\n",
    "    x_model = transf.fit_transform(x_model)\n",
    "    y_model = clf.predict(x_model)\n",
    "    x_test = X_test\n",
    "    fig = plt.figure(figsize = (20,10))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "    ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "    ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "    plt.legend()\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I am inputting certain values of lambda to find the best among them using CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_cv(np.array([0.0005, 0.001, 0.01, 0.1, 1, 10, 100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Further precision can be obtained in the values of lambda by inputting values around 0.0005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_cv(np.array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
